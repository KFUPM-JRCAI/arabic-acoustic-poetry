{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GithHub_code_runner_sinai_finetued_signal_classification_source_code.ipynb","provenance":[{"file_id":"https://github.com/MagedSaeed/arabic-poetry-speech-classification/blob/main/signal-classification/runner.ipynb","timestamp":1636031715689}],"collapsed_sections":[],"machine_shape":"hm"},"interpreter":{"hash":"cb837155d4bf090da51172bd16385c4d352f77e83b230ed743050e46498701be"},"kernelspec":{"display_name":"Python 3.8.10 64-bit ('arabic-speech-poetry-classification': virtualenv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5ec0b9c8610b4559b8b32ea364d8f889":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8d60dfc988f3422dbd14b49d54d3be71","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4beb2ec094064545bc235aa549df9d65","IPY_MODEL_10f801461ee44f37905a5b220ba3392a","IPY_MODEL_67dd087128e440bdbb6948567ce2629e"]}},"8d60dfc988f3422dbd14b49d54d3be71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4beb2ec094064545bc235aa549df9d65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5afe6223f3b747baac30fb707ace8cd3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d70deb2f65364662aa47fbfb437b8c63"}},"10f801461ee44f37905a5b220ba3392a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c0d180043e434cc3a3feb5728fe55cda","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2934,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2934,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_af44fc9157434fa6935e649fcc1eefc8"}},"67dd087128e440bdbb6948567ce2629e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1cb4bc6beeac433cbde99dc66ad05807","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2934/2934 [00:47&lt;00:00, 61.15ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_179b6e35185e49f3bbd873c7e00f6c92"}},"5afe6223f3b747baac30fb707ace8cd3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d70deb2f65364662aa47fbfb437b8c63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c0d180043e434cc3a3feb5728fe55cda":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"af44fc9157434fa6935e649fcc1eefc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1cb4bc6beeac433cbde99dc66ad05807":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"179b6e35185e49f3bbd873c7e00f6c92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d110250bbcd5442dafc2da979683820e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f7a4e137c1fe4b50b32176448626cb60","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ea90212307bb4645ae5745dc8432371b","IPY_MODEL_30cc567398b24c2dbfe68876255ca89f","IPY_MODEL_571113ea0e4f406e88fa3fa52f344b38"]}},"f7a4e137c1fe4b50b32176448626cb60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ea90212307bb4645ae5745dc8432371b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d801f3d6bbd54f87915fe1f43c70b48c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7b4fb4f3b9ed497b9f5668443d8be507"}},"30cc567398b24c2dbfe68876255ca89f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b16c02dfcda24c36b4331bc2e5fbbb14","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":367,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":367,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6088e7651b424a5cb8f70da8cd7314d1"}},"571113ea0e4f406e88fa3fa52f344b38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ecd12e9688b04a1999b493e4be8e72c4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 367/367 [00:05&lt;00:00, 63.84ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_241a4dcde1db49e3866bedfed28f3782"}},"d801f3d6bbd54f87915fe1f43c70b48c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7b4fb4f3b9ed497b9f5668443d8be507":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b16c02dfcda24c36b4331bc2e5fbbb14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6088e7651b424a5cb8f70da8cd7314d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ecd12e9688b04a1999b493e4be8e72c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"241a4dcde1db49e3866bedfed28f3782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f826187f8ec440ac90cc66a0ed817d68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e705e86264c742d2a9aa6c0c805f26b9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_83be268e69af4084a71e98ab591a3dc2","IPY_MODEL_19d61badbbc84e778144b5169d094664","IPY_MODEL_6352921d15cd480b9f7e3013f006d028"]}},"e705e86264c742d2a9aa6c0c805f26b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"83be268e69af4084a71e98ab591a3dc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_570f8aa7412e4fbfb5667d919fdebd21","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_161f313ae5b84ae188999371d8c63420"}},"19d61badbbc84e778144b5169d094664":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e77e835f11ef4854b0419506cb10f7d8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":367,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":367,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_134d226ac8c444c9a297546de2dd2caf"}},"6352921d15cd480b9f7e3013f006d028":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a1a3de4c18a14597b37629a4baf98a1b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 367/367 [00:05&lt;00:00, 66.36ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_da99cb3347124086ae80c57f2a9af42a"}},"570f8aa7412e4fbfb5667d919fdebd21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"161f313ae5b84ae188999371d8c63420":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e77e835f11ef4854b0419506cb10f7d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"134d226ac8c444c9a297546de2dd2caf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1a3de4c18a14597b37629a4baf98a1b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"da99cb3347124086ae80c57f2a9af42a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7e36bc3e23c4445d96e6b2f77b832083":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b48c22dfc3af4cef8f61405bd46ab336","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bc2d8476fc6249778b7b8d1c0dddb965","IPY_MODEL_ed20601d892745f0b1ee41c2ea419b53","IPY_MODEL_0b0d2c5515344d9bbb409bfb86ba23aa"]}},"b48c22dfc3af4cef8f61405bd46ab336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc2d8476fc6249778b7b8d1c0dddb965":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_55d4e52e38144ef3ae706d8a0b46e15a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a6452784378344bd8117753c3a2eaf3d"}},"ed20601d892745f0b1ee41c2ea419b53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_53f777706e634f419668b7839997a70b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1467,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1467,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_23bbeeffa1c846179c78e6e027dc0922"}},"0b0d2c5515344d9bbb409bfb86ba23aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1462d6f392e34f6daa7753aa678e21ae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1467/1467 [02:49&lt;00:00,  8.40ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2324fe8df3b8461a9d5971609791a131"}},"55d4e52e38144ef3ae706d8a0b46e15a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a6452784378344bd8117753c3a2eaf3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"53f777706e634f419668b7839997a70b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"23bbeeffa1c846179c78e6e027dc0922":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1462d6f392e34f6daa7753aa678e21ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2324fe8df3b8461a9d5971609791a131":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8145fb7fbd0742c68466e8e34ff1b0c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_80bc35fd009b468dab81e802dba2618c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1841fb9f763b4dca8552864976600dda","IPY_MODEL_2268f493f0cc4763896ed14f4b20c051","IPY_MODEL_7467d723304a492d97860e3ee46647b3"]}},"80bc35fd009b468dab81e802dba2618c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1841fb9f763b4dca8552864976600dda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_874fe467fbef4154982d7eee64b76ebc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9d0f10bc5ec948c8a0d18d42b2101d3c"}},"2268f493f0cc4763896ed14f4b20c051":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5a5f633540bb4076ae851dcab481f88a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":184,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":184,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ec62122fa564724907b62d8052c2f23"}},"7467d723304a492d97860e3ee46647b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_42c8e93b2c6a4359a602e26cab90c2ae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 184/184 [00:21&lt;00:00,  9.78ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e1c2275e697f4e8b886369b0fead8fbf"}},"874fe467fbef4154982d7eee64b76ebc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9d0f10bc5ec948c8a0d18d42b2101d3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a5f633540bb4076ae851dcab481f88a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5ec62122fa564724907b62d8052c2f23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42c8e93b2c6a4359a602e26cab90c2ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e1c2275e697f4e8b886369b0fead8fbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"81499199433f45eea7cbbab44ce7475f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bc3ff62b29be4bbd8faf860c9fbfe4af","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_19dd1b2b6a614424a4d18edb95e7b112","IPY_MODEL_895cee1a7c324925add6bf4abd37c6bb","IPY_MODEL_53cbc6e8dcc64ebc82469c5a6d116a18"]}},"bc3ff62b29be4bbd8faf860c9fbfe4af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19dd1b2b6a614424a4d18edb95e7b112":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9acdd8e191924b609656a64eb224be88","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_999644af315e4716a26162ba6b2a7092"}},"895cee1a7c324925add6bf4abd37c6bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6231a2523f774dfcb1f3ffa4b9d2c31d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":184,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":184,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_64269e128a6c4e4688e95037b6a0d98c"}},"53cbc6e8dcc64ebc82469c5a6d116a18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5a62473eeec14d7fbb41c50311622f32","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 184/184 [00:21&lt;00:00,  8.90ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d3da648645274f5eac68e064caa1bbc8"}},"9acdd8e191924b609656a64eb224be88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"999644af315e4716a26162ba6b2a7092":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6231a2523f774dfcb1f3ffa4b9d2c31d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"64269e128a6c4e4688e95037b6a0d98c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a62473eeec14d7fbb41c50311622f32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d3da648645274f5eac68e064caa1bbc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"exRwNYwC3Onr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638552640956,"user_tz":-180,"elapsed":1093,"user":{"displayName":"Maged Saeed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghfl38dqLtgiKi2-s-gKPRT0lr9gXQd7UDqCF22lA=s64","userId":"08011552846066909361"}},"outputId":"6e3ed92a-e26f-41a2-b562-979c5df672e0"},"source":["%cd /content\n","# remove first if exists\n","!rm -rf arabic-poetry-speech-classification\n","!git clone https://github.com/MagedSaeed/arabic-poetry-speech-classification.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'arabic-poetry-speech-classification'...\n","remote: Enumerating objects: 43, done.\u001b[K\n","remote: Counting objects: 100% (43/43), done.\u001b[K\n","remote: Compressing objects: 100% (31/31), done.\u001b[K\n","remote: Total 43 (delta 19), reused 31 (delta 11), pack-reused 0\u001b[K\n","Unpacking objects: 100% (43/43), done.\n"]}]},{"cell_type":"code","metadata":{"id":"4dhFl6M-44uV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638552661640,"user_tz":-180,"elapsed":20689,"user":{"displayName":"Maged Saeed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghfl38dqLtgiKi2-s-gKPRT0lr9gXQd7UDqCF22lA=s64","userId":"08011552846066909361"}},"outputId":"5dde8c4a-a893-4752-9df8-f1aaa5ed1aad"},"source":["# mount the drive to get the datasets\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"YZOMr-Bn4jI2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638552661641,"user_tz":-180,"elapsed":19,"user":{"displayName":"Maged Saeed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghfl38dqLtgiKi2-s-gKPRT0lr9gXQd7UDqCF22lA=s64","userId":"08011552846066909361"}},"outputId":"6bb370c8-4154-4347-fed9-3676781b8e0a"},"source":["# put the task path you want to work on here\n","%cd arabic-poetry-speech-classification/signal-classification"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/arabic-poetry-speech-classification/signal-classification\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLZPLV-qAmQq","executionInfo":{"status":"ok","timestamp":1638552709731,"user_tz":-180,"elapsed":48099,"user":{"displayName":"Maged Saeed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghfl38dqLtgiKi2-s-gKPRT0lr9gXQd7UDqCF22lA=s64","userId":"08011552846066909361"}},"outputId":"e4418f8e-07db-4f52-851c-d0c4e167020b"},"source":["!pip install torchaudio\n","!pip install transformers\n","!pip install datasets\n","!pip install lang_trans\n","!pip install arabic_reshaper\n","!pip install python-bidi\n","!pip install pydub\n","!pip install soundfile\n","!pip install jiwer==2.2.0\n","!pip install PyArabic"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n","Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchaudio) (3.10.0.2)\n","Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 5.0 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 51.9 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 51.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 433 kB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 46.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n","Collecting datasets\n","  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n","\u001b[K     |████████████████████████████████| 298 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 51.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 65.8 MB/s \n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 46.4 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 59.3 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 55.5 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 48.5 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.16.1 frozenlist-1.2.0 fsspec-2021.11.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n","Collecting lang_trans\n","  Downloading lang-trans-0.6.0.tar.gz (5.7 kB)\n","Building wheels for collected packages: lang-trans\n","  Building wheel for lang-trans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lang-trans: filename=lang_trans-0.6.0-py3-none-any.whl size=6345 sha256=9aa91dbd73b1b7c438a4e1c9a64fe10c180fce1ee20b28d260184ee16771e63a\n","  Stored in directory: /root/.cache/pip/wheels/ec/57/fe/a8a1df3409a81b298f4969f3e3084fe840033d7c03aec8e9e5\n","Successfully built lang-trans\n","Installing collected packages: lang-trans\n","Successfully installed lang-trans-0.6.0\n","Collecting arabic_reshaper\n","  Downloading arabic_reshaper-2.1.3-py3-none-any.whl (20 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from arabic_reshaper) (57.4.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from arabic_reshaper) (0.16.0)\n","Installing collected packages: arabic-reshaper\n","Successfully installed arabic-reshaper-2.1.3\n","Collecting python-bidi\n","  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-bidi) (1.15.0)\n","Installing collected packages: python-bidi\n","Successfully installed python-bidi-0.4.2\n","Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub\n","Successfully installed pydub-0.25.1\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (0.10.3.post1)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile) (2.21)\n","Collecting jiwer==2.2.0\n","  Downloading jiwer-2.2.0-py3-none-any.whl (13 kB)\n","Collecting python-Levenshtein\n","  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jiwer==2.2.0) (1.19.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->jiwer==2.2.0) (57.4.0)\n","Building wheels for collected packages: python-Levenshtein\n","  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149851 sha256=a8f3237b68706e7f99a3cb83777c1725c2be691453bc001980763f78f4fd6bd9\n","  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n","Successfully built python-Levenshtein\n","Installing collected packages: python-Levenshtein, jiwer\n","Successfully installed jiwer-2.2.0 python-Levenshtein-0.12.2\n","Collecting PyArabic\n","  Downloading PyArabic-0.6.14-py3-none-any.whl (126 kB)\n","\u001b[K     |████████████████████████████████| 126 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from PyArabic) (1.15.0)\n","Installing collected packages: PyArabic\n","Successfully installed PyArabic-0.6.14\n"]}]},{"cell_type":"code","metadata":{"id":"7eFqbmnnAL-n"},"source":["import os\n","import re\n","import sys\n","import json\n","import torch\n","import jiwer\n","import logging\n","import librosa\n","import datasets\n","import itertools\n","import torchaudio\n","import numpy as np\n","import transformers\n","import pandas as pd\n","from torch import nn\n","import seaborn as sns\n","from tqdm import tqdm\n","import torch.nn as nn\n","import soundfile as sf\n","import arabic_reshaper\n","from pyarabic import araby\n","from packaging import version\n","from pydub import AudioSegment\n","from dataclasses import asdict\n","from trainer import CTCTrainer\n","import matplotlib.pyplot as plt\n","from pydub.utils import mediainfo\n","from argparse import ArgumentParser\n","from collections import defaultdict\n","from torch.nn import functional as F\n","from contextlib import contextmanager\n","from bidi.algorithm import get_display\n","from lang_trans.arabic import buckwalter\n","from dataclasses import dataclass, field\n","from datasets import load_dataset, Dataset\n","from sklearn.metrics import accuracy_score\n","from models import Wav2Vec2ClassificationModel\n","from processors import CustomWav2Vec2Processor\n","from typing import Any, Dict, List, Optional, Union\n","from transformers import HfArgumentParser,TrainingArguments\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.metrics import classification_report, confusion_matrix\n","from transformers.trainer_utils import get_last_checkpoint, is_main_process\n","from transformers import is_apex_available,set_seed ,Trainer,Wav2Vec2FeatureExtractor\n","from arg_parsers import DataTrainingArguments, ModelArguments, DataCollatorCTCWithPadding\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2Model,Wav2Vec2PreTrainedModel\n","% matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4BretRa_kHc"},"source":["!cp -r /content/drive/MyDrive/KFUPM-Master/ICS606/Dataset/All_poems.zip ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWT8mSb1ADRU"},"source":["if os.path.exists('dataset'):\n","  if len(os.listdir('dataset')) == 0:\n","    os.system('unzip All_poems.zip -d dataset')\n","else:\n","  os.system('unzip All_poems.zip -d dataset')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0W2S-XtAGhi"},"source":["!mkdir -p dataset_wav"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G7f5oPvDCx4L"},"source":["!cp /content/drive/MyDrive/KFUPM-Master/ICS606/Dataset/testset.csv .\n","!cp /content/drive/MyDrive/KFUPM-Master/ICS606/Dataset/trainset.csv .\n","!cp /content/drive/MyDrive/KFUPM-Master/ICS606/Dataset/valset.csv ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jv2ZeRsUAIF9"},"source":["metadata_test_path = 'testset.csv'\n","metadata_train_path = 'trainset.csv'\n","metadata_val_path = 'valset.csv'\n","dataset_folder = 'dataset'\n","dataset_wav_folder = 'dataset_wav'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fytnRuUYAwnr"},"source":["train_metadata = pd.read_csv(metadata_train_path)\n","test_metadata = pd.read_csv(metadata_test_path)\n","val_metadata = pd.read_csv(metadata_val_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_KVXrgqA11f","executionInfo":{"status":"ok","timestamp":1638553347652,"user_tz":-180,"elapsed":606043,"user":{"displayName":"Maged Saeed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghfl38dqLtgiKi2-s-gKPRT0lr9gXQd7UDqCF22lA=s64","userId":"08011552846066909361"}},"outputId":"d1ea2304-2953-40f9-c9ea-00de15ef8f85"},"source":["sample_rates = set()\n","for file_path in tqdm(list(itertools.chain(\n","    train_metadata['Utterance name'],\n","    test_metadata['Utterance name'],\n","    val_metadata['Utterance name']\n","  ))):\n","  complete_path = f'{dataset_folder}/{file_path}'\n","  complete_wav_path = f'{dataset_wav_folder}/{file_path}'\n","  # os.system(f'ffmpeg -i {complete_path} {complete_wav_path}')\n","  audio = AudioSegment.from_file(complete_path)\n","  sample_rates.add(audio.frame_rate)\n","  audio.export(f'{dataset_wav_folder}/{file_path}', format='wav')\n","sample_rates"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3668/3668 [10:05<00:00,  6.05it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["{44100, 48000}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"LjctTq9YGN0b"},"source":["os.environ[\"WANDB_DISABLED\"] = \"true\"\n","if is_apex_available():\n","    from apex import amp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5vkbv2L22um"},"source":["if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n","    _is_native_amp_available = True\n","    from torch.cuda.amp import autocast\n","\n","logger = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9OGE9OMu22um"},"source":["## getting training arguments"]},{"cell_type":"code","metadata":{"id":"ZbA7jzkl6M_e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638553347654,"user_tz":-180,"elapsed":13,"user":{"displayName":"Maged Saeed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghfl38dqLtgiKi2-s-gKPRT0lr9gXQd7UDqCF22lA=s64","userId":"08011552846066909361"}},"outputId":"2dfe2cc4-e3d9-4f42-ab26-d9322c83f683"},"source":["# testing path poetry-classification\n","# model_output_path = '--output_dir=poetry-classification'\n","args_string=\"\"\"\n","--model_name_or_path=\"bakrianoo/sinai-voice-ar-stt\"\n","--output_dir=/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned\n","--freeze_feature_extractor \n","--num_train_epochs=\"5\" \n","--per_device_train_batch_size=\"2\" \n","--preprocessing_num_workers=\"1\" \n","--learning_rate=\"3e-5\" \n","--warmup_steps=\"5\" \n","--evaluation_strategy=\"steps\"\n","--save_steps=\"500\" \n","--eval_steps=\"100\" \n","--save_total_limit=\"1\" \n","--logging_steps=\"50\" \n","--do_eval=1\n","--do_train=1\n","\"\"\"\n","def match_numeric_type(s):\n","  try:\n","    return json.loads(s)\n","  except:\n","    return s\n","\n","# construct the args dict from the args string\n","args_dict = dict()\n","for line in args_string.splitlines():\n","  line = line.replace('--','').replace(r'\\\\','').replace('\"','').replace(\"'\",'').strip()\n","  if line:\n","    if '=' in line:\n","      arg_name,arg_value = line.split('=')\n","      args_dict[arg_name] = arg_value\n","    else:\n","      args_dict[line] = ''\n","\n","# match args to dataclasses\n","args_class_attrs = defaultdict(list)\n","for arg_name,arg_value in args_dict.items():\n","  for ArgClass in (ModelArguments, DataTrainingArguments, TrainingArguments):\n","    if arg_name in ArgClass.__annotations__:\n","      args_class_attrs[ArgClass].append(arg_name)\n","\n","# initialzie args objects\n","model_args = ModelArguments(**{arg_name:match_numeric_type(args_dict[arg_name]) for arg_name in args_class_attrs[ModelArguments]})\n","data_args = DataTrainingArguments(**{arg_name:match_numeric_type(args_dict[arg_name]) for arg_name in args_class_attrs[DataTrainingArguments]})\n","training_args = TrainingArguments(**{arg_name:match_numeric_type(args_dict[arg_name]) for arg_name in args_class_attrs[TrainingArguments]})"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]}]},{"cell_type":"markdown","metadata":{"id":"l8rZGxSD22up"},"source":["## logging setup"]},{"cell_type":"code","metadata":{"id":"qQx95tK422uq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638553347655,"user_tz":-180,"elapsed":10,"user":{"displayName":"Maged Saeed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghfl38dqLtgiKi2-s-gKPRT0lr9gXQd7UDqCF22lA=s64","userId":"08011552846066909361"}},"outputId":"1c5fd32a-2a94-460a-f9e7-0c15947a8f0b"},"source":["# Setup logging\n","logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","    datefmt=\"%m/%d/%Y %H:%M:%S\",\n","    handlers=[logging.StreamHandler(sys.stdout)],\n",")\n","# logger.setLevel(\n","#     logging.INFO if is_main_process(training_args.local_rank) else logging.WARN\n","# )\n","logger.setLevel(\n","    logging.WARN\n",")\n","# Log on each process the small summary:\n","logger.warning(\n","    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",")\n","# Set the verbosity to info of the Transformers logger (on main process only):\n","if is_main_process(training_args.local_rank):\n","    transformers.utils.logging.set_verbosity_info()\n","\n","# Set seed before initializing model.\n","set_seed(training_args.seed)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12/03/2021 17:42:27 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n"]}]},{"cell_type":"markdown","metadata":{"id":"A8YlSTxl22us"},"source":["## dataset processing"]},{"cell_type":"code","metadata":{"id":"75eh-pi322us","colab":{"base_uri":"https://localhost:8080/","height":297,"referenced_widgets":["5ec0b9c8610b4559b8b32ea364d8f889","8d60dfc988f3422dbd14b49d54d3be71","4beb2ec094064545bc235aa549df9d65","10f801461ee44f37905a5b220ba3392a","67dd087128e440bdbb6948567ce2629e","5afe6223f3b747baac30fb707ace8cd3","d70deb2f65364662aa47fbfb437b8c63","c0d180043e434cc3a3feb5728fe55cda","af44fc9157434fa6935e649fcc1eefc8","1cb4bc6beeac433cbde99dc66ad05807","179b6e35185e49f3bbd873c7e00f6c92","d110250bbcd5442dafc2da979683820e","f7a4e137c1fe4b50b32176448626cb60","ea90212307bb4645ae5745dc8432371b","30cc567398b24c2dbfe68876255ca89f","571113ea0e4f406e88fa3fa52f344b38","d801f3d6bbd54f87915fe1f43c70b48c","7b4fb4f3b9ed497b9f5668443d8be507","b16c02dfcda24c36b4331bc2e5fbbb14","6088e7651b424a5cb8f70da8cd7314d1","ecd12e9688b04a1999b493e4be8e72c4","241a4dcde1db49e3866bedfed28f3782","f826187f8ec440ac90cc66a0ed817d68","e705e86264c742d2a9aa6c0c805f26b9","83be268e69af4084a71e98ab591a3dc2","19d61badbbc84e778144b5169d094664","6352921d15cd480b9f7e3013f006d028","570f8aa7412e4fbfb5667d919fdebd21","161f313ae5b84ae188999371d8c63420","e77e835f11ef4854b0419506cb10f7d8","134d226ac8c444c9a297546de2dd2caf","a1a3de4c18a14597b37629a4baf98a1b","da99cb3347124086ae80c57f2a9af42a","7e36bc3e23c4445d96e6b2f77b832083","b48c22dfc3af4cef8f61405bd46ab336","bc2d8476fc6249778b7b8d1c0dddb965","ed20601d892745f0b1ee41c2ea419b53","0b0d2c5515344d9bbb409bfb86ba23aa","55d4e52e38144ef3ae706d8a0b46e15a","a6452784378344bd8117753c3a2eaf3d","53f777706e634f419668b7839997a70b","23bbeeffa1c846179c78e6e027dc0922","1462d6f392e34f6daa7753aa678e21ae","2324fe8df3b8461a9d5971609791a131","8145fb7fbd0742c68466e8e34ff1b0c0","80bc35fd009b468dab81e802dba2618c","1841fb9f763b4dca8552864976600dda","2268f493f0cc4763896ed14f4b20c051","7467d723304a492d97860e3ee46647b3","874fe467fbef4154982d7eee64b76ebc","9d0f10bc5ec948c8a0d18d42b2101d3c","5a5f633540bb4076ae851dcab481f88a","5ec62122fa564724907b62d8052c2f23","42c8e93b2c6a4359a602e26cab90c2ae","e1c2275e697f4e8b886369b0fead8fbf","81499199433f45eea7cbbab44ce7475f","bc3ff62b29be4bbd8faf860c9fbfe4af","19dd1b2b6a614424a4d18edb95e7b112","895cee1a7c324925add6bf4abd37c6bb","53cbc6e8dcc64ebc82469c5a6d116a18","9acdd8e191924b609656a64eb224be88","999644af315e4716a26162ba6b2a7092","6231a2523f774dfcb1f3ffa4b9d2c31d","64269e128a6c4e4688e95037b6a0d98c","5a62473eeec14d7fbb41c50311622f32","d3da648645274f5eac68e064caa1bbc8"]},"executionInfo":{"status":"ok","timestamp":1638553627016,"user_tz":-180,"elapsed":279368,"user":{"displayName":"Maged Saeed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghfl38dqLtgiKi2-s-gKPRT0lr9gXQd7UDqCF22lA=s64","userId":"08011552846066909361"}},"outputId":"d2f63586-52fc-444b-cd83-1438d54cedc0"},"source":["train_metadata = pd.read_csv(metadata_train_path)\n","val_metadata = pd.read_csv(metadata_val_path)\n","test_metadata = pd.read_csv(metadata_test_path)\n","\n","train_dataset = Dataset.from_pandas(train_metadata)\n","eval_dataset = Dataset.from_pandas(val_metadata)\n","test_dataset = Dataset.from_pandas(test_metadata)\n","\n","if data_args.max_train_samples is not None:\n","    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","\n","if data_args.max_val_samples is not None:\n","    eval_dataset = eval_dataset.select(range(data_args.max_val_samples))\n","# Preprocessing the datasets.\n","# We need to read the aduio files as arrays and tokenize the targets.\n","resamplers = {  # The dataset contains all the uncommented sample rates\n","    48000: torchaudio.transforms.Resample(48000, 16000),\n","    44100: torchaudio.transforms.Resample(44100, 16000),\n","    # 32000: torchaudio.transforms.Resample(32000, 16000),\n","}\n","\n","labels = {\n","    bahr: bahr_index\n","    for bahr_index, bahr in enumerate(sorted(set(train_metadata[\"Bahr\"])))\n","}\n","print(\"labels are:\", labels)\n","print(\"len:\", len(labels))\n","\n","def speech_file_to_array_fn(batch):\n","    start = 0\n","    stop = 20\n","    srate = 16_000\n","    speech_array, sampling_rate = torchaudio.load(\n","        f'dataset_wav/{batch[\"Utterance name\"]}'\n","    )\n","    speech_array = speech_array[0]\n","    batch[\"speech\"] = resamplers[sampling_rate](speech_array).squeeze().numpy()\n","    batch[\"sampling_rate\"] = srate\n","    batch[\"parent\"] = labels[batch[\"Bahr\"]]\n","    return batch\n","\n","train_dataset = train_dataset.map(\n","    speech_file_to_array_fn,\n","    remove_columns=train_dataset.column_names,\n","    num_proc=data_args.preprocessing_num_workers,\n",")\n","eval_dataset = eval_dataset.map(\n","    speech_file_to_array_fn,\n","    remove_columns=eval_dataset.column_names,\n","    num_proc=data_args.preprocessing_num_workers,\n",")\n","\n","test_dataset = test_dataset.map(\n","    speech_file_to_array_fn,\n","    remove_columns=test_dataset.column_names,\n","    num_proc=data_args.preprocessing_num_workers,\n","\n",")\n","\n","feature_extractor = Wav2Vec2FeatureExtractor(\n","    feature_size=1,\n","    sampling_rate=16_000,\n","    padding_value=0.0,\n","    do_normalize=True,\n","    return_attention_mask=True,\n",")\n","processor = CustomWav2Vec2Processor(feature_extractor=feature_extractor)\n","\n","def prepare_dataset(batch):\n","    # check that all files have the correct sampling rate\n","    assert (\n","        len(set(batch[\"sampling_rate\"])) == 1\n","    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n","    batch[\"input_values\"] = processor(\n","        batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]\n","    ).input_values\n","    batch[\"labels\"] = batch[\"parent\"]\n","    return batch\n","\n","train_dataset = train_dataset.map(\n","    prepare_dataset,\n","    remove_columns=train_dataset.column_names,\n","    batch_size=training_args.per_device_train_batch_size,\n","    batched=True,\n","    num_proc=data_args.preprocessing_num_workers,\n",")\n","eval_dataset = eval_dataset.map(\n","    prepare_dataset,\n","    remove_columns=eval_dataset.column_names,\n","    batch_size=training_args.per_device_train_batch_size,\n","    batched=True,\n","    num_proc=data_args.preprocessing_num_workers,\n",")\n","test_dataset = test_dataset.map(\n","    prepare_dataset,\n","    remove_columns=test_dataset.column_names,\n","    batch_size=training_args.per_device_train_batch_size,\n","    batched=True,\n","    num_proc=data_args.preprocessing_num_workers,\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["labels are: {'البسيط': 0, 'الخفيف': 1, 'الرجز': 2, 'الرمل': 3, 'السريع': 4, 'الطويل': 5, 'الكامل': 6, 'المتدارك': 7, 'المتقارب': 8, 'المجتث': 9, 'المديد': 10, 'المضارع': 11, 'المقتضب': 12, 'المنسرح': 13, 'الهزج': 14, 'الوافر': 15}\n","len: 16\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ec0b9c8610b4559b8b32ea364d8f889","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2934 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d110250bbcd5442dafc2da979683820e","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/367 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f826187f8ec440ac90cc66a0ed817d68","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/367 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e36bc3e23c4445d96e6b2f77b832083","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1467 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8145fb7fbd0742c68466e8e34ff1b0c0","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/184 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81499199433f45eea7cbbab44ce7475f","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/184 [00:00<?, ?ba/s]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Vi4VHERr22ur"},"source":["## check for previous checkpoints"]},{"cell_type":"code","metadata":{"id":"33v3mSrX22ur"},"source":["# Detecting last checkpoint.\n","last_checkpoint = None\n","if (\n","    os.path.isdir(training_args.output_dir)\n","    and training_args.do_train\n","    and not training_args.overwrite_output_dir\n","):\n","    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","        raise ValueError(\n","            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","            \"Use --overwrite_output_dir to overcome.\"\n","        )\n","    elif last_checkpoint is not None:\n","        logger.info(\n","            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PtcJCvUcOOZL"},"source":["## prepare the model"]},{"cell_type":"code","metadata":{"id":"7ReQKRpCOROb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638553666788,"user_tz":-180,"elapsed":39796,"user":{"displayName":"Maged Saeed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghfl38dqLtgiKi2-s-gKPRT0lr9gXQd7UDqCF22lA=s64","userId":"08011552846066909361"}},"outputId":"1b614289-4495-4a2a-edc6-4ad0fae59258"},"source":["pretrained_model_path = '/content/drive/MyDrive/KFUPM-Master/ICS606/Models/SinaiFineTuned/checkpoint-1600'\n","model = Wav2Vec2ClassificationModel.from_pretrained(\n","    pretrained_model_path,\n","    attention_dropout=0.01,\n","    hidden_dropout=0.01,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.01,\n","    num_attention_heads=4,\n",")\n","\n","if model_args.freeze_feature_extractor:\n","    model.freeze_feature_extractor()\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids.argmax(-1)\n","    preds = pred.predictions.argmax(-1)\n","    acc = accuracy_score(labels, preds)\n","    report = classification_report(labels, preds)\n","    matrix = confusion_matrix(labels, preds)\n","    # print(matrix)\n","    # print('clasification report',report)\n","    # print('accuracy',acc)\n","    return {\"accuracy\": acc}\n","\n","# Data collator\n","data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n","# Initialize our Trainer\n","trainer = CTCTrainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset if training_args.do_train else None,\n","    eval_dataset=eval_dataset if training_args.do_eval else None,\n","    tokenizer=processor.feature_extractor,\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/KFUPM-Master/ICS606/Models/SinaiFineTuned/checkpoint-1600/config.json\n","Model config Wav2Vec2Config {\n","  \"_name_or_path\": \"bakrianoo/sinai-voice-ar-stt\",\n","  \"activation_dropout\": 0.0,\n","  \"apply_spec_augment\": true,\n","  \"architectures\": [\n","    \"Wav2Vec2ForCTC\"\n","  ],\n","  \"attention_dropout\": 0.01,\n","  \"bos_token_id\": 1,\n","  \"classifier_proj_size\": 256,\n","  \"codevector_dim\": 256,\n","  \"contrastive_logits_temperature\": 0.1,\n","  \"conv_bias\": true,\n","  \"conv_dim\": [\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512\n","  ],\n","  \"conv_kernel\": [\n","    10,\n","    3,\n","    3,\n","    3,\n","    3,\n","    2,\n","    2\n","  ],\n","  \"conv_stride\": [\n","    5,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"ctc_loss_reduction\": \"mean\",\n","  \"ctc_zero_infinity\": false,\n","  \"diversity_loss_weight\": 0.1,\n","  \"do_stable_layer_norm\": true,\n","  \"eos_token_id\": 2,\n","  \"feat_extract_activation\": \"gelu\",\n","  \"feat_extract_dropout\": 0.0,\n","  \"feat_extract_norm\": \"layer\",\n","  \"feat_proj_dropout\": 0.0,\n","  \"feat_quantizer_dropout\": 0.0,\n","  \"final_dropout\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.01,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-05,\n","  \"layerdrop\": 0.01,\n","  \"mask_channel_length\": 10,\n","  \"mask_channel_min_space\": 1,\n","  \"mask_channel_other\": 0.0,\n","  \"mask_channel_prob\": 0.0,\n","  \"mask_channel_selection\": \"static\",\n","  \"mask_feature_length\": 10,\n","  \"mask_feature_prob\": 0.0,\n","  \"mask_time_length\": 10,\n","  \"mask_time_min_space\": 1,\n","  \"mask_time_other\": 0.0,\n","  \"mask_time_prob\": 0.05,\n","  \"mask_time_selection\": \"static\",\n","  \"model_type\": \"wav2vec2\",\n","  \"num_attention_heads\": 4,\n","  \"num_codevector_groups\": 2,\n","  \"num_codevectors_per_group\": 320,\n","  \"num_conv_pos_embedding_groups\": 16,\n","  \"num_conv_pos_embeddings\": 128,\n","  \"num_feat_extract_layers\": 7,\n","  \"num_hidden_layers\": 24,\n","  \"num_negatives\": 100,\n","  \"pad_token_id\": 45,\n","  \"proj_codevector_dim\": 256,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"use_weighted_layer_sum\": false,\n","  \"vocab_size\": 48\n","}\n","\n","loading weights file /content/drive/MyDrive/KFUPM-Master/ICS606/Models/SinaiFineTuned/checkpoint-1600/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/KFUPM-Master/ICS606/Models/SinaiFineTuned/checkpoint-1600 were not used when initializing Wav2Vec2ClassificationModel: ['lm_head.weight', 'lm_head.bias']\n","- This IS expected if you are initializing Wav2Vec2ClassificationModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing Wav2Vec2ClassificationModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of Wav2Vec2ClassificationModel were not initialized from the model checkpoint at /content/drive/MyDrive/KFUPM-Master/ICS606/Models/SinaiFineTuned/checkpoint-1600 and are newly initialized: ['linear2.weight', 'linear1.weight', 'linear2.bias', 'linear1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","metadata":{"id":"JOqrkIEP22ut"},"source":["## training"]},{"cell_type":"code","metadata":{"id":"x3MGBP4X22ut","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1638570240877,"user_tz":-180,"elapsed":16574130,"user":{"displayName":"Maged Saeed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghfl38dqLtgiKi2-s-gKPRT0lr9gXQd7UDqCF22lA=s64","userId":"08011552846066909361"}},"outputId":"36fe7499-b69a-487d-e67c-0265d119059c"},"source":["# Training\n","if training_args.do_train:\n","  if last_checkpoint is not None:\n","      checkpoint = last_checkpoint\n","  elif os.path.isdir(model_args.model_name_or_path):\n","      checkpoint = model_args.model_name_or_path\n","  else:\n","      checkpoint = None\n","  train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","  trainer.save_model()\n","\n","  # save the feature_extractor and the tokenizer\n","  if is_main_process(training_args.local_rank):\n","      processor.save_pretrained(training_args.output_dir)\n","\n","  metrics = train_result.metrics\n","  max_train_samples = (\n","      data_args.max_train_samples\n","      if data_args.max_train_samples is not None\n","      else len(train_dataset)\n","  )\n","  metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","  trainer.log_metrics(\"train\", metrics)\n","  trainer.save_metrics(\"train\", metrics)\n","  trainer.save_state()\n","\n","# Evaluation\n","results = {}\n","if training_args.do_eval:\n","  logger.info(\"*** Evaluate ***\")\n","  metrics = trainer.evaluate()\n","  max_val_samples = (\n","      data_args.max_val_samples\n","      if data_args.max_val_samples is not None\n","      else len(eval_dataset)\n","  )\n","  metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n","\n","  trainer.log_metrics(\"eval\", metrics)\n","  trainer.save_metrics(\"eval\", metrics)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 2934\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 2\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7335\n","/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  tensor = as_tensor(value)\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6944' max='7335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6944/7335 4:36:08 < 15:33, 0.42 it/s, Epoch 4.73/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>2.581300</td>\n","      <td>2.247297</td>\n","      <td>0.297003</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>2.259300</td>\n","      <td>2.241882</td>\n","      <td>0.256131</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>2.110200</td>\n","      <td>1.721183</td>\n","      <td>0.490463</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.760300</td>\n","      <td>1.735919</td>\n","      <td>0.558583</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.578800</td>\n","      <td>1.336853</td>\n","      <td>0.607629</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.423800</td>\n","      <td>1.469066</td>\n","      <td>0.580381</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.411600</td>\n","      <td>1.304375</td>\n","      <td>0.626703</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.205200</td>\n","      <td>1.595283</td>\n","      <td>0.550409</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.249400</td>\n","      <td>0.914040</td>\n","      <td>0.713896</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.317300</td>\n","      <td>0.975664</td>\n","      <td>0.705722</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.653700</td>\n","      <td>1.207339</td>\n","      <td>0.702997</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>1.208300</td>\n","      <td>1.389351</td>\n","      <td>0.673025</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>1.097400</td>\n","      <td>1.239107</td>\n","      <td>0.702997</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>1.172600</td>\n","      <td>1.121745</td>\n","      <td>0.776567</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.526800</td>\n","      <td>0.836769</td>\n","      <td>0.776567</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.455700</td>\n","      <td>0.800246</td>\n","      <td>0.784741</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.617800</td>\n","      <td>0.784525</td>\n","      <td>0.822888</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.407100</td>\n","      <td>0.775465</td>\n","      <td>0.828338</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.594800</td>\n","      <td>0.678423</td>\n","      <td>0.841962</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.435900</td>\n","      <td>0.749941</td>\n","      <td>0.858311</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.230900</td>\n","      <td>0.711427</td>\n","      <td>0.847411</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.674600</td>\n","      <td>0.594185</td>\n","      <td>0.871935</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.248300</td>\n","      <td>0.604906</td>\n","      <td>0.880109</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.757600</td>\n","      <td>0.465201</td>\n","      <td>0.899183</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.433800</td>\n","      <td>0.574943</td>\n","      <td>0.852861</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.406400</td>\n","      <td>0.524283</td>\n","      <td>0.880109</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.621600</td>\n","      <td>0.644245</td>\n","      <td>0.871935</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.394500</td>\n","      <td>0.490618</td>\n","      <td>0.893733</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.442700</td>\n","      <td>0.362077</td>\n","      <td>0.912807</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.121500</td>\n","      <td>0.388777</td>\n","      <td>0.918256</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.067900</td>\n","      <td>0.389117</td>\n","      <td>0.904632</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.066900</td>\n","      <td>0.432082</td>\n","      <td>0.907357</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.033400</td>\n","      <td>0.521309</td>\n","      <td>0.888283</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.152400</td>\n","      <td>0.649411</td>\n","      <td>0.893733</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.052000</td>\n","      <td>0.609554</td>\n","      <td>0.874659</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.222600</td>\n","      <td>0.437700</td>\n","      <td>0.912807</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.030000</td>\n","      <td>0.440668</td>\n","      <td>0.934605</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.134500</td>\n","      <td>0.465650</td>\n","      <td>0.915531</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>0.211100</td>\n","      <td>0.350753</td>\n","      <td>0.940054</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.113500</td>\n","      <td>0.596897</td>\n","      <td>0.907357</td>\n","    </tr>\n","    <tr>\n","      <td>4100</td>\n","      <td>0.059500</td>\n","      <td>0.526237</td>\n","      <td>0.910082</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.122100</td>\n","      <td>0.463114</td>\n","      <td>0.923706</td>\n","    </tr>\n","    <tr>\n","      <td>4300</td>\n","      <td>0.098100</td>\n","      <td>0.375868</td>\n","      <td>0.940054</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>0.014800</td>\n","      <td>0.473047</td>\n","      <td>0.931880</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.011700</td>\n","      <td>0.425426</td>\n","      <td>0.929155</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>0.066800</td>\n","      <td>0.430566</td>\n","      <td>0.920981</td>\n","    </tr>\n","    <tr>\n","      <td>4700</td>\n","      <td>0.019700</td>\n","      <td>0.422849</td>\n","      <td>0.934605</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.034500</td>\n","      <td>0.368095</td>\n","      <td>0.931880</td>\n","    </tr>\n","    <tr>\n","      <td>4900</td>\n","      <td>0.023700</td>\n","      <td>0.301974</td>\n","      <td>0.937330</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.193200</td>\n","      <td>0.353514</td>\n","      <td>0.942779</td>\n","    </tr>\n","    <tr>\n","      <td>5100</td>\n","      <td>0.045700</td>\n","      <td>0.331609</td>\n","      <td>0.929155</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>0.000500</td>\n","      <td>0.300497</td>\n","      <td>0.937330</td>\n","    </tr>\n","    <tr>\n","      <td>5300</td>\n","      <td>0.047100</td>\n","      <td>0.283302</td>\n","      <td>0.942779</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>0.034100</td>\n","      <td>0.324021</td>\n","      <td>0.942779</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.029900</td>\n","      <td>0.303333</td>\n","      <td>0.948229</td>\n","    </tr>\n","    <tr>\n","      <td>5600</td>\n","      <td>0.001800</td>\n","      <td>0.280856</td>\n","      <td>0.937330</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.006300</td>\n","      <td>0.323527</td>\n","      <td>0.937330</td>\n","    </tr>\n","    <tr>\n","      <td>5800</td>\n","      <td>0.000500</td>\n","      <td>0.377622</td>\n","      <td>0.931880</td>\n","    </tr>\n","    <tr>\n","      <td>5900</td>\n","      <td>0.000200</td>\n","      <td>0.333306</td>\n","      <td>0.934605</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.093000</td>\n","      <td>0.318518</td>\n","      <td>0.942779</td>\n","    </tr>\n","    <tr>\n","      <td>6100</td>\n","      <td>0.000100</td>\n","      <td>0.329285</td>\n","      <td>0.934605</td>\n","    </tr>\n","    <tr>\n","      <td>6200</td>\n","      <td>0.069500</td>\n","      <td>0.287853</td>\n","      <td>0.956403</td>\n","    </tr>\n","    <tr>\n","      <td>6300</td>\n","      <td>0.000300</td>\n","      <td>0.278690</td>\n","      <td>0.948229</td>\n","    </tr>\n","    <tr>\n","      <td>6400</td>\n","      <td>0.004400</td>\n","      <td>0.277931</td>\n","      <td>0.950954</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.000000</td>\n","      <td>0.283405</td>\n","      <td>0.950954</td>\n","    </tr>\n","    <tr>\n","      <td>6600</td>\n","      <td>0.000100</td>\n","      <td>0.286160</td>\n","      <td>0.956403</td>\n","    </tr>\n","    <tr>\n","      <td>6700</td>\n","      <td>0.016600</td>\n","      <td>0.281632</td>\n","      <td>0.956403</td>\n","    </tr>\n","    <tr>\n","      <td>6800</td>\n","      <td>0.009400</td>\n","      <td>0.302228</td>\n","      <td>0.950954</td>\n","    </tr>\n","    <tr>\n","      <td>6900</td>\n","      <td>0.002800</td>\n","      <td>0.315597</td>\n","      <td>0.948229</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-500\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-500/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-500/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-500/preprocessor_config.json\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-1000\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-1000/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-1000/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-1000/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-1500\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-1500/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-1500/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-1500/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-1000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-2000\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-2000/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-2000/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-2000/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-1500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-2500\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-2500/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-2500/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-2500/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-2000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-3000\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-3000/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-3000/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-3000/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-2500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-3500\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-3500/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-3500/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-3500/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-3000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-4000\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-4000/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-4000/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-4000/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-3500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-4500\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-4500/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-4500/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-4500/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-4000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-5000\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-5000/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-5000/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-5000/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-4500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-5500\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-5500/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-5500/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-5500/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-5000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-6000\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-6000/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-6000/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-6000/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-5500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-6500\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-6500/config.json\n","Model weights saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-6500/pytorch_model.bin\n","Configuration saved in /content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-6500/preprocessor_config.json\n","Deleting older checkpoint [/content/drive/MyDrive/KFUPM-Master/ICS606/Models/ClassificationModelSinaiFineTuned/checkpoint-6000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n","***** Running Evaluation *****\n","  Num examples = 367\n","  Batch size = 8\n","/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:986: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  return (input_length - kernel_size) // stride + 1\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-2b9de3506f6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 if (\n","\u001b[0;32m/content/arabic-poetry-speech-classification/signal-classification/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"2H7FhX4yNLuG"},"source":["# use it if you face problems of checkpoints not being saved\n","from google.colab import drive\n","drive.flush_and_unmount()"],"execution_count":null,"outputs":[]}]}